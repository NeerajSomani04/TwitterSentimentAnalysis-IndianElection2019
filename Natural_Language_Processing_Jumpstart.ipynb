{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JumpStart: Natural  Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** A general introduction to basic NLP methods to serve as a foundation for further self study and additional NLP curriculums. This notebook intends to serve as a basis for various techniques such as text processing, information retrieval, and classifying text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Reddit dataset we will use today comes from Google BigQuery. You can find it [here](https://bigquery.cloud.google.com/table/fh-bigquery:reddit_posts.2018_05?pli=1).\n",
    " - The data is public but you need to have an active account on Google Clound Platform first in order to access it.\n",
    "- The original data was huge so we sampled it from the top 10 subreddit.\n",
    "\n",
    "- We will also learn the following NLP packages in Python along the way\n",
    "\n",
    " - [NLTK](http://www.nltk.org/) - a very popular package for doing NLP in Python\n",
    "\n",
    " - [Textblob](https://textblob.readthedocs.io/en/dev/) - similar to NLTK but provides a higher level API for easy accessing.\n",
    "\n",
    " - [WordCloud](https://github.com/amueller/word_cloud) - how to run wordcloud in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Open your **Terminal/Anaconda Prompt**, cd to the lecture code folder and run the following command:\n",
    " - `pip install -r requirements.txt`\n",
    "\n",
    "- After installing all the required packages, run the following command:\n",
    " - `python -m textblob.download_corpora`\n",
    " \n",
    "- Restart this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/neerajsomani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/neerajsomani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Uncomment the following line the first time you run the code\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://s3.amazonaws.com/nycdsabt01/reddit_top10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is always a good idea to check the shape of the dataframe and column types before you run any type of operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737339, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc      int64\n",
       "subreddit       object\n",
       "author          object\n",
       "domain          object\n",
       "url             object\n",
       "num_comments     int64\n",
       "score            int64\n",
       "ups              int64\n",
       "downs            int64\n",
       "title           object\n",
       "selftext        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you first readin a dataset, I would recommend using `df.sample()` rather than `df.head()` because sometimes the first couple rows are fine, however, there might be missing values or mixed types in the column so it is better if you can get a big picture of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>718669</th>\n",
       "      <td>1525321842</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>miamiherald.com</td>\n",
       "      <td>http://www.miamiherald.com/news/business/artic...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Business] - NY state starts 3rd round of dron...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298185</th>\n",
       "      <td>1526472466</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>theguardian.com</td>\n",
       "      <td>https://www.theguardian.com/environment/2018/m...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Environment] - One man's race to capture the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133116</th>\n",
       "      <td>1525471275</td>\n",
       "      <td>RocketLeagueExchange</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.RocketLeagueExchange</td>\n",
       "      <td>https://www.reddit.com/r/RocketLeagueExchange/...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Xbox][H]nc import[W]offers</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710626</th>\n",
       "      <td>1525349528</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>AdolphEinstien</td>\n",
       "      <td>zerohedge.com</td>\n",
       "      <td>https://www.zerohedge.com/news/2018-05-02/hill...</td>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>Hillary Clinton Now Blaming Socialist Democrat...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468816</th>\n",
       "      <td>1527306636</td>\n",
       "      <td>Ice_Poseidon</td>\n",
       "      <td>MagnoliaKing</td>\n",
       "      <td>self.Ice_Poseidon</td>\n",
       "      <td>https://www.reddit.com/r/Ice_Poseidon/comments...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Asian Andy trying to appeal to his 10 yr old f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93532</th>\n",
       "      <td>1527104168</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.Showerthoughts</td>\n",
       "      <td>https://www.reddit.com/r/Showerthoughts/commen...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Raccoons are like a cross between a dog and a ...</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429701</th>\n",
       "      <td>1526852189</td>\n",
       "      <td>ACCIDENTAL_HAIKU_BOT</td>\n",
       "      <td>ACCIDENTAL_HAIKU_BOT</td>\n",
       "      <td>self.ACCIDENTAL_HAIKU_BOT</td>\n",
       "      <td>https://www.reddit.com/r/ACCIDENTAL_HAIKU_BOT/...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/u/JunglePygmy's accidental haiku in /r/advent...</td>\n",
       "      <td>Can we PLEASE get a \\n    gigantic gorgeou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219413</th>\n",
       "      <td>1527451868</td>\n",
       "      <td>newsbotbot</td>\n",
       "      <td>-en-</td>\n",
       "      <td>mobile.twitter.com</td>\n",
       "      <td>https://mobile.twitter.com/CBSNews/status/1000...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@CBSNews: \"I'm just overwhelmed with gratitude...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329256</th>\n",
       "      <td>1527520452</td>\n",
       "      <td>FortNiteBR</td>\n",
       "      <td>iOnlyReadPussy</td>\n",
       "      <td>v.redd.it</td>\n",
       "      <td>https://v.redd.it/n4d9p1ot4m011</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Yall underestimating us, mobile players too much</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556179</th>\n",
       "      <td>1525958693</td>\n",
       "      <td>Ice_Poseidon</td>\n",
       "      <td>PrlvateClient</td>\n",
       "      <td>gyazo.com</td>\n",
       "      <td>https://gyazo.com/f34e28e93a66b351b27b8aea04c4...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>TTD</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc             subreddit                author  \\\n",
       "718669   1525321842         AutoNewspaper    AutoNewspaperAdmin   \n",
       "298185   1526472466         AutoNewspaper    AutoNewspaperAdmin   \n",
       "133116   1525471275  RocketLeagueExchange             [deleted]   \n",
       "710626   1525349528            The_Donald        AdolphEinstien   \n",
       "468816   1527306636          Ice_Poseidon          MagnoliaKing   \n",
       "93532    1527104168        Showerthoughts             [deleted]   \n",
       "429701   1526852189  ACCIDENTAL_HAIKU_BOT  ACCIDENTAL_HAIKU_BOT   \n",
       "219413   1527451868            newsbotbot                  -en-   \n",
       "329256   1527520452            FortNiteBR        iOnlyReadPussy   \n",
       "556179   1525958693          Ice_Poseidon         PrlvateClient   \n",
       "\n",
       "                           domain  \\\n",
       "718669            miamiherald.com   \n",
       "298185            theguardian.com   \n",
       "133116  self.RocketLeagueExchange   \n",
       "710626              zerohedge.com   \n",
       "468816          self.Ice_Poseidon   \n",
       "93532         self.Showerthoughts   \n",
       "429701  self.ACCIDENTAL_HAIKU_BOT   \n",
       "219413         mobile.twitter.com   \n",
       "329256                  v.redd.it   \n",
       "556179                  gyazo.com   \n",
       "\n",
       "                                                      url  num_comments  \\\n",
       "718669  http://www.miamiherald.com/news/business/artic...             0   \n",
       "298185  https://www.theguardian.com/environment/2018/m...             0   \n",
       "133116  https://www.reddit.com/r/RocketLeagueExchange/...             4   \n",
       "710626  https://www.zerohedge.com/news/2018-05-02/hill...            13   \n",
       "468816  https://www.reddit.com/r/Ice_Poseidon/comments...             1   \n",
       "93532   https://www.reddit.com/r/Showerthoughts/commen...             3   \n",
       "429701  https://www.reddit.com/r/ACCIDENTAL_HAIKU_BOT/...             0   \n",
       "219413  https://mobile.twitter.com/CBSNews/status/1000...             0   \n",
       "329256                    https://v.redd.it/n4d9p1ot4m011            26   \n",
       "556179  https://gyazo.com/f34e28e93a66b351b27b8aea04c4...             0   \n",
       "\n",
       "        score  ups  downs                                              title  \\\n",
       "718669      1    1      0  [Business] - NY state starts 3rd round of dron...   \n",
       "298185      2    2      0  [Environment] - One man's race to capture the ...   \n",
       "133116      1    1      0                        [Xbox][H]nc import[W]offers   \n",
       "710626    153  153      0  Hillary Clinton Now Blaming Socialist Democrat...   \n",
       "468816      5    5      0  Asian Andy trying to appeal to his 10 yr old f...   \n",
       "93532       0    0      0  Raccoons are like a cross between a dog and a ...   \n",
       "429701      1    1      0  /u/JunglePygmy's accidental haiku in /r/advent...   \n",
       "219413      1    1      0  @CBSNews: \"I'm just overwhelmed with gratitude...   \n",
       "329256     20   20      0   Yall underestimating us, mobile players too much   \n",
       "556179      1    1      0                                                TTD   \n",
       "\n",
       "                                                 selftext  \n",
       "718669                                                NaN  \n",
       "298185                                                NaN  \n",
       "133116                                          [deleted]  \n",
       "710626                                                NaN  \n",
       "468816                                                NaN  \n",
       "93532                                           [deleted]  \n",
       "429701      Can we PLEASE get a \\n    gigantic gorgeou...  \n",
       "219413                                                NaN  \n",
       "329256                                                NaN  \n",
       "556179                                                NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `selftext` is the raw text of each Reddit post. But take a look at the column. There are missing values, `[deleted]`, `[removed]` which should not be considered as valid text.\n",
    "- We need to clean the text before we can further analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na with empty string\n",
    "df['selftext'] = df['selftext'].fillna('')\n",
    "# Replace `removed` and `deleted` with empty string\n",
    "tbr = ['[removed]', '[deleted]']\n",
    "df['selftext'] = df['selftext'].apply(lambda x: '' if x in tbr else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After cleansing the data, about 88% of our `selftext` column are just empty string.\n",
    "- It makes sense to concatenate the text with its title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8806152936437649\n"
     ]
    }
   ],
   "source": [
    "print(sum(df['selftext'] == '') / df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = df['title'] + ' ' + df['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>666144</th>\n",
       "      <td>1526876429</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>ThinkerPlus</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8k...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>How do you turn a Winnebago into a Sex Winnebago?</td>\n",
       "      <td>How do you turn a Winnebago into a Sex Winneba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221554</th>\n",
       "      <td>1527432621</td>\n",
       "      <td>Ice_Poseidon</td>\n",
       "      <td>BeardedDelight</td>\n",
       "      <td>self.Ice_Poseidon</td>\n",
       "      <td>https://www.reddit.com/r/Ice_Poseidon/comments...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Look at this 8AM content for the EU fags God b...</td>\n",
       "      <td>Look at this 8AM content for the EU fags God b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70758</th>\n",
       "      <td>1527102344</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>Desmoire</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8l...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the pettiest thing you have done (or h...</td>\n",
       "      <td>What is the pettiest thing you have done (or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>1527119675</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>DEWFOUR</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8l...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>What food, unique to your country, should food...</td>\n",
       "      <td>What food, unique to your country, should food...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214828</th>\n",
       "      <td>1527452366</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>zaphodsheads</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Fakers of reddit, what happened when the docto...</td>\n",
       "      <td>Fakers of reddit, what happened when the docto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266370</th>\n",
       "      <td>1526407089</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>csmonitor.com</td>\n",
       "      <td>https://www.csmonitor.com/USA/Politics/2018/05...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[National] - Legislators work to shift culture...</td>\n",
       "      <td>[National] - Legislators work to shift culture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194395</th>\n",
       "      <td>1526248395</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>tampabay.com</td>\n",
       "      <td>http://www.tampabay.com/news/business/Trump-pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Local] - Trump pledges to help Chinese phone ...</td>\n",
       "      <td>[Local] - Trump pledges to help Chinese phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13317</th>\n",
       "      <td>1526728874</td>\n",
       "      <td>Ice_Poseidon</td>\n",
       "      <td>zorroisreal</td>\n",
       "      <td>imgur.com</td>\n",
       "      <td>https://imgur.com/vXTtGrB</td>\n",
       "      <td>3</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>Petition for Brother Harry to be on the Next R...</td>\n",
       "      <td>Petition for Brother Harry to be on the Next R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542303</th>\n",
       "      <td>1525912204</td>\n",
       "      <td>RocketLeagueExchange</td>\n",
       "      <td>pcpresident2016</td>\n",
       "      <td>self.RocketLeagueExchange</td>\n",
       "      <td>https://www.reddit.com/r/RocketLeagueExchange/...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[xbox] [H] Triumph crates [W] unpainted exotic...</td>\n",
       "      <td>[xbox] [H] Triumph crates [W] unpainted exotic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393548</th>\n",
       "      <td>1525542530</td>\n",
       "      <td>me_irl</td>\n",
       "      <td>MeWhoBelievesInYou</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://i.redd.it/0ak0rt5or2w01.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>me_irl</td>\n",
       "      <td>me_irl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc             subreddit              author  \\\n",
       "666144   1526876429             AskReddit         ThinkerPlus   \n",
       "221554   1527432621          Ice_Poseidon      BeardedDelight   \n",
       "70758    1527102344             AskReddit            Desmoire   \n",
       "99974    1527119675             AskReddit             DEWFOUR   \n",
       "214828   1527452366             AskReddit        zaphodsheads   \n",
       "266370   1526407089         AutoNewspaper  AutoNewspaperAdmin   \n",
       "194395   1526248395         AutoNewspaper  AutoNewspaperAdmin   \n",
       "13317    1526728874          Ice_Poseidon         zorroisreal   \n",
       "542303   1525912204  RocketLeagueExchange     pcpresident2016   \n",
       "393548   1525542530                me_irl  MeWhoBelievesInYou   \n",
       "\n",
       "                           domain  \\\n",
       "666144             self.AskReddit   \n",
       "221554          self.Ice_Poseidon   \n",
       "70758              self.AskReddit   \n",
       "99974              self.AskReddit   \n",
       "214828             self.AskReddit   \n",
       "266370              csmonitor.com   \n",
       "194395               tampabay.com   \n",
       "13317                   imgur.com   \n",
       "542303  self.RocketLeagueExchange   \n",
       "393548                  i.redd.it   \n",
       "\n",
       "                                                      url  num_comments  \\\n",
       "666144  https://www.reddit.com/r/AskReddit/comments/8k...            14   \n",
       "221554  https://www.reddit.com/r/Ice_Poseidon/comments...             1   \n",
       "70758   https://www.reddit.com/r/AskReddit/comments/8l...             4   \n",
       "99974   https://www.reddit.com/r/AskReddit/comments/8l...             1   \n",
       "214828  https://www.reddit.com/r/AskReddit/comments/8m...             0   \n",
       "266370  https://www.csmonitor.com/USA/Politics/2018/05...             0   \n",
       "194395  http://www.tampabay.com/news/business/Trump-pl...             0   \n",
       "13317                           https://imgur.com/vXTtGrB             3   \n",
       "542303  https://www.reddit.com/r/RocketLeagueExchange/...             5   \n",
       "393548                https://i.redd.it/0ak0rt5or2w01.jpg             0   \n",
       "\n",
       "        score  ups  downs                                              title  \\\n",
       "666144      1    1      0  How do you turn a Winnebago into a Sex Winnebago?   \n",
       "221554      2    2      0  Look at this 8AM content for the EU fags God b...   \n",
       "70758       2    2      0  What is the pettiest thing you have done (or h...   \n",
       "99974       1    1      0  What food, unique to your country, should food...   \n",
       "214828      1    1      0  Fakers of reddit, what happened when the docto...   \n",
       "266370      1    1      0  [National] - Legislators work to shift culture...   \n",
       "194395      1    1      0  [Local] - Trump pledges to help Chinese phone ...   \n",
       "13317     176  176      0  Petition for Brother Harry to be on the Next R...   \n",
       "542303      0    0      0  [xbox] [H] Triumph crates [W] unpainted exotic...   \n",
       "393548      6    6      0                                             me_irl   \n",
       "\n",
       "                                                 selftext  \n",
       "666144  How do you turn a Winnebago into a Sex Winneba...  \n",
       "221554  Look at this 8AM content for the EU fags God b...  \n",
       "70758   What is the pettiest thing you have done (or h...  \n",
       "99974   What food, unique to your country, should food...  \n",
       "214828  Fakers of reddit, what happened when the docto...  \n",
       "266370  [National] - Legislators work to shift culture...  \n",
       "194395  [Local] - Trump pledges to help Chinese phone ...  \n",
       "13317   Petition for Brother Harry to be on the Next R...  \n",
       "542303  [xbox] [H] Triumph crates [W] unpainted exotic...  \n",
       "393548                                            me_irl   "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Convert all the text to lowercase - avoids having multiple copies of the same words.\n",
    "- Replace url in the text with empty space.\n",
    "- Replace all empty spaces with just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Convert all the string to lower cases\n",
    "df['selftext'] = df['selftext'].str.lower()\n",
    "# \\S+ means anything that is not an empty space\n",
    "df['selftext'] = df['selftext'].apply(lambda x: re.sub('http\\S*', '', x))\n",
    "# \\s+ means all empty space (\\n, \\r, \\t)\n",
    "df['selftext'] = df['selftext'].apply(lambda x: re.sub('\\s+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take a look at the dataframe after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>628114</th>\n",
       "      <td>1527647133</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>macredsmile</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>https://www.youtube.com/watch?v=mD6Iqxegug4</td>\n",
       "      <td>5</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>VICTORY: Tommy Robinson Reporting Restrictions...</td>\n",
       "      <td>victory: tommy robinson reporting restrictions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724680</th>\n",
       "      <td>1525384876</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>ktbs.com</td>\n",
       "      <td>https://www.ktbs.com/news/arklatex-indepth/ark...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[National] - ArkLaTex real estate sales buckin...</td>\n",
       "      <td>[national] - arklatex real estate sales buckin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476371</th>\n",
       "      <td>1527306533</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>nbcnews.com</td>\n",
       "      <td>https://www.nbcnews.com/news/us-news/jury-reco...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Business] - Jury recommends $25M in Johnson &amp;...</td>\n",
       "      <td>[business] - jury recommends $25m in johnson &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713934</th>\n",
       "      <td>1525372693</td>\n",
       "      <td>newsbotbot</td>\n",
       "      <td>-en-</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>https://twitter.com/AFP/status/992109284447010817</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@AFP: #WorldPressFreedomDay Journalists march ...</td>\n",
       "      <td>@afp: #worldpressfreedomday journalists march ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196870</th>\n",
       "      <td>1526192219</td>\n",
       "      <td>FortNiteBR</td>\n",
       "      <td>Insonarc</td>\n",
       "      <td>v.redd.it</td>\n",
       "      <td>https://v.redd.it/r545qoq9fkx01</td>\n",
       "      <td>45</td>\n",
       "      <td>945</td>\n",
       "      <td>945</td>\n",
       "      <td>0</td>\n",
       "      <td>\"No Skins\" are the nicest skins</td>\n",
       "      <td>\"no skins\" are the nicest skins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551615</th>\n",
       "      <td>1525981136</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>Ta_Ta_Toothie</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>https://twitter.com/funder/status/994637881753...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>BREAKING: Nunes and staff now under investigat...</td>\n",
       "      <td>breaking: nunes and staff now under investigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99823</th>\n",
       "      <td>1527115407</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8l...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Kids these days, what new music should I be li...</td>\n",
       "      <td>kids these days, what new music should i be li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648944</th>\n",
       "      <td>1527717254</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/8n...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>If you were forced to cut off one part of your...</td>\n",
       "      <td>if you were forced to cut off one part of your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17575</th>\n",
       "      <td>1526759295</td>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>AutoNewspaperAdmin</td>\n",
       "      <td>france24.com</td>\n",
       "      <td>http://www.france24.com/en/20180519-halep-down...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[World] - Halep downs Sharapova to set up Rome...</td>\n",
       "      <td>[world] - halep downs sharapova to set up rome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651875</th>\n",
       "      <td>1527656072</td>\n",
       "      <td>Ice_Poseidon</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>self.Ice_Poseidon</td>\n",
       "      <td>https://www.reddit.com/r/Ice_Poseidon/comments...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Walking up to a drug deal LOL</td>\n",
       "      <td>walking up to a drug deal lol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc      subreddit              author             domain  \\\n",
       "628114   1527647133     The_Donald         macredsmile        youtube.com   \n",
       "724680   1525384876  AutoNewspaper  AutoNewspaperAdmin           ktbs.com   \n",
       "476371   1527306533  AutoNewspaper  AutoNewspaperAdmin        nbcnews.com   \n",
       "713934   1525372693     newsbotbot                -en-        twitter.com   \n",
       "196870   1526192219     FortNiteBR            Insonarc          v.redd.it   \n",
       "551615   1525981136     The_Donald       Ta_Ta_Toothie        twitter.com   \n",
       "99823    1527115407      AskReddit           [deleted]     self.AskReddit   \n",
       "648944   1527717254      AskReddit           [deleted]     self.AskReddit   \n",
       "17575    1526759295  AutoNewspaper  AutoNewspaperAdmin       france24.com   \n",
       "651875   1527656072   Ice_Poseidon           [deleted]  self.Ice_Poseidon   \n",
       "\n",
       "                                                      url  num_comments  \\\n",
       "628114        https://www.youtube.com/watch?v=mD6Iqxegug4             5   \n",
       "724680  https://www.ktbs.com/news/arklatex-indepth/ark...             0   \n",
       "476371  https://www.nbcnews.com/news/us-news/jury-reco...             0   \n",
       "713934  https://twitter.com/AFP/status/992109284447010817             0   \n",
       "196870                    https://v.redd.it/r545qoq9fkx01            45   \n",
       "551615  https://twitter.com/funder/status/994637881753...             1   \n",
       "99823   https://www.reddit.com/r/AskReddit/comments/8l...             1   \n",
       "648944  https://www.reddit.com/r/AskReddit/comments/8n...             6   \n",
       "17575   http://www.france24.com/en/20180519-halep-down...             0   \n",
       "651875  https://www.reddit.com/r/Ice_Poseidon/comments...             0   \n",
       "\n",
       "        score  ups  downs                                              title  \\\n",
       "628114    212  212      0  VICTORY: Tommy Robinson Reporting Restrictions...   \n",
       "724680      1    1      0  [National] - ArkLaTex real estate sales buckin...   \n",
       "476371      1    1      0  [Business] - Jury recommends $25M in Johnson &...   \n",
       "713934      1    1      0  @AFP: #WorldPressFreedomDay Journalists march ...   \n",
       "196870    945  945      0                    \"No Skins\" are the nicest skins   \n",
       "551615      1    1      0  BREAKING: Nunes and staff now under investigat...   \n",
       "99823       1    1      0  Kids these days, what new music should I be li...   \n",
       "648944      1    1      0  If you were forced to cut off one part of your...   \n",
       "17575       1    1      0  [World] - Halep downs Sharapova to set up Rome...   \n",
       "651875      1    1      0                      Walking up to a drug deal LOL   \n",
       "\n",
       "                                                 selftext  \n",
       "628114  victory: tommy robinson reporting restrictions...  \n",
       "724680  [national] - arklatex real estate sales buckin...  \n",
       "476371  [business] - jury recommends $25m in johnson &...  \n",
       "713934  @afp: #worldpressfreedomday journalists march ...  \n",
       "196870                   \"no skins\" are the nicest skins   \n",
       "551615  breaking: nunes and staff now under investigat...  \n",
       "99823   kids these days, what new music should i be li...  \n",
       "648944  if you were forced to cut off one part of your...  \n",
       "17575   [world] - halep downs sharapova to set up rome...  \n",
       "651875                     walking up to a drug deal lol   "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Steps and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before we start using machine learning methods on our text, there are some steps that we first want to perform so that our text is in a format that our model can interpret.\n",
    "- These steps include:\n",
    " - Filtering\n",
    " - Tokenization\n",
    " - Stemming\n",
    " - Lemmitization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first step is to remove punctuation, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = df['selftext'].apply(lambda x: re.sub('[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When examining a text, often there are words used within a sentence that holds no meaning for various data mining operations such as topic modeling or word frequency. \n",
    "    - Examples of this include \"the\", \"is\", etc. Collectively, these are known as \"stopwords\". \n",
    "- When mining for certain information, you should note whether your method should remove certain stopwords (for example, wordclouds). To illustrate an example, we will call upon the stopwords method from nltk. \n",
    "- Note, methods that interact with the text itself is usually found under nltk.corpus. Corpus is the linguistics term for set of structured text used for statistical study so be mindful of this specific vocabulary.\n",
    "- The stop words from nltk is just a Python list so you can easily append more stopwords to it. For example \"computer\" would be a stopword in corpus largely dealing with data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = df['selftext'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization is the act of splitting text into a sequence of words. In this example, we will try a simplistic tokenization method below using the standard split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'toy', 'example.', 'Illustrate', 'this', 'example', 'below.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is a toy example. Illustrate this example below.\"\n",
    "sample_tokens = sample_text.split()\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Did you notice something? While we have the tokens, \"example\" and \"example.\" are treated as different tokens. As a NLP data scientist, you must make the choice on whether you choose to distinguish the two.\n",
    "\n",
    "- Note, various packages in Python such as the nltk package will default tokenize \".\" as a seperate token instead to designate it it's own special meaning. This can be illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'toy',\n",
       " 'example',\n",
       " '.',\n",
       " 'Illustrate',\n",
       " 'this',\n",
       " 'example',\n",
       " 'below',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- However, textblob treats \".\" just as a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['This', 'is', 'a', 'toy', 'example', 'Illustrate', 'this', 'example', 'below'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "TextBlob(sample_text).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Various words in English have the same meaning. There are two main methods for handling tasks such as recognizing \"strike, striking, struck\" as the same words.\n",
    "\n",
    "- Stemming refers to the removal of suffixes, like “ing”, “ly”, “s”, etc. by a simple rule-based approach.\n",
    "\n",
    "- The most common stemming algorithms are:\n",
    " - [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/) (the older traditional method)\n",
    " - [Lancaster Stemmer](http://textanalysisonline.com/nltk-lancaster-stemmer) (a more aggressive modern stemmer)\n",
    "\n",
    "- Stemming and lemmatization can both be done with self written rules using creative forms of regex but for practical example demo in this notebook, we will implement the PorterStemmer method from nltk on the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonprocess_text = \"I am writing a Python string\"\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text = ' '.join([stemmer.stem(word) for word in nonprocess_text.split()])\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: This is more robust than the standard regex implementation as we see here \"writing\" is converted to \"write\" but \"string\" isn't converted to \"stre\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike stemming, lemmatization will try to identify root words that are semantically similar to text based off a dictionary corpus. In essence, you can think of being able to replicate the effect manually by implementing a look-up method after parsing a text. Therefore, we usually prefer using lemmatization over stemming.\n",
    "\n",
    "- There are various dictionaries one can use to base lemmization off of. NLTK's [wordnet](http://wordnet.princeton.edu/) is quite powerful to handle most lemmatization task. We'll examine a few implementations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemztr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemztr.lemmatize('feet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note, lemmatization will return back the string if the text isn't found in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemztr.lemmatize('abacadabradoo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "\n",
    "- Unigrams do not usually contain as much information as compared to bigrams and trigrams. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. \n",
    "\n",
    "- The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.\n",
    "\n",
    "- Google hosts its n-gram corpora on [AWS S3](https://aws.amazon.com/datasets/google-books-ngrams/) for free. \n",
    "- The size of the file is about 2.2TB. You might consider using [Python API](https://github.com/dimazest/google-ngram-downloader)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams - Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(df['selftext'][5]).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can easily implement the N-gram function using native Python - it is a common nlp interview question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "find_ngrams(input_list, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=2000, width=800, height=400)\n",
    "# generate word cloud\n",
    "wc.generate(' '.join(df['selftext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This wordcloud is generated using all the text data. However, it makes more sense to have a separate wordcloud for each individual subreddit.\n",
    "- If you find any frequent word that doesn't contain useful information, you should consider adding it to your stopword list.\n",
    "- You can find more examples on the [documentation](http://amueller.github.io/word_cloud/auto_examples/index.html) and [blog post](http://minimaxir.com/2016/05/wordclouds/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment analysis refers to the use of natural language processing, text analysis, and computational linguistics to identify emotional states and subjective information.\n",
    "\n",
    "- Using sentiment analysis, we can gain information about the attitude of the speaker or writer of text with respect to the topic. \n",
    "\n",
    "- Today we will just call the [sentiment analysis API](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis) from TextBlob and take it like a black box as we haven't talked about machine learning yet.\n",
    "\n",
    "- We want to apply the function to the text column of the dataframe and generate two new columns called polarity and subjectivity. The process will take a long time so we will apply it to a sample of dataset.\n",
    "    - Polarity refers to the emotions expressed in the text.\n",
    "    - Subjectivity is a measure of how subjective vs objective the text is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's use sentiment analysis to analyze the relationship between polarity and number of thumb ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out all posts that have less than 100 upvotes\n",
    "sa_df = df.loc[df.ups > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "def sentiment_func(x):\n",
    "    sentiment = TextBlob(x['selftext'])\n",
    "    x['polarity'] = sentiment.polarity\n",
    "    x['subjectivity'] = sentiment.subjectivity\n",
    "    return x\n",
    "\n",
    "sample = sa_df.sample(sample_size).apply(sentiment_func, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.plot.scatter('ups', 'polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Resources:\n",
    "\n",
    "Other advanced libraries in Python that we will cover in the future lectures:\n",
    "\n",
    "[Spacy](https://spacy.io/)\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
